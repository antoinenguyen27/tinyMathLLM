{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package, Model and Dataset instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Antoi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Antoi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "cuda available? True\n",
      "device name: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "import random\n",
    "import re\n",
    "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig, TrlParser\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training, LoftQConfig\n",
    "import types\n",
    "\n",
    "\n",
    "model_path = \"C:/Users/Antoi/documents/resumeProjects/minimathllm/fuckItWeBall/huginn-0125_checkpoint/huginn-0125\"\n",
    "\n",
    "training_set_path = \"C:/Users/Antoi/documents/resumeProjects/minimathllm/data/openmath1/OpenMathInstruct-1_train.csv\"\n",
    "test_set_path = \"C:/Users/Antoi/documents/resumeProjects/minimathllm/data/openmath1/OpenMathInstruct-1_validation.csv\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "training_set_df = pd.read_csv(training_set_path)\n",
    "test_set_df = pd.read_csv(test_set_path)\n",
    "\n",
    "training_set_df = training_set_df.rename(columns={\"question\": \"prompt\"})\n",
    "test_set_df = test_set_df.rename(columns={\"question\": \"prompt\"})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"cuda available? \" + str(torch.cuda.is_available()))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_set = Dataset.from_pandas(training_set_df)\n",
    "test_set = Dataset.from_pandas(test_set_df)\n",
    "\n",
    "print(\"device name: \" + str(torch.cuda.get_device_name()))\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of reward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rewards(completions, **kwargs):\n",
    "    rewards = []\n",
    "\n",
    "    for completion in completions:\n",
    "        try:\n",
    "            pattern = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\s*Thus the answer is: \\boxed{([\\s\\S]*?)}$\"\n",
    "            match = re.search(pattern, completion, re.DOTALL)\n",
    "\n",
    "            if match is None or len(match.groups()) != 2:\n",
    "                rewards.append(0.0)\n",
    "            else:\n",
    "                rewards.append(1.0)\n",
    "\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "            \n",
    "    return rewards\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_rewards(completions,expected_answer, **kwargs):\n",
    "    rewards = []\n",
    "\n",
    "    for completion, answer in zip(completions, expected_answer):\n",
    "        try: \n",
    "\n",
    "            if (random.random()) <= 0.01:\n",
    "                print(\"Completion Length: \" + str(len(completion)))\n",
    "            answerFormatPattern = r\"Thus the answer is: \\boxed{([\\s\\S]*?)}$\"\n",
    "            answerFormatMatch = re.search(answerFormatPattern, completion, re.DOTALL)\n",
    "\n",
    "            if answerFormatMatch is None: \n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            answerExtractionPattern = r\"\\\\boxed{([\\s\\S]*?)}\"\n",
    "            answerExtractionMatch = re.search(answerExtractionPattern, completion)\n",
    "\n",
    "            if answerExtractionMatch is None:\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "\n",
    "            if answerExtractionMatch.group(1).strip() == str(answer):\n",
    "                rewards.append(1.0)\n",
    "\n",
    "            else:\n",
    "                rewards.append(0.0)\n",
    "\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate prompt to allow model CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5553234/5553234 [04:14<00:00, 21835.78 examples/s]\n",
      "Map: 100%|██████████| 847570/847570 [00:38<00:00, 22229.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_CoT_prompt(question):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \n",
    "                     \"content\" : \"You are a helpful mathematician. You first think about the reasoning process in your mind and then provide the user with the answer.\"})\n",
    "    messages.append({\"role\": \"user\", \"content\" : f\"Showing full working out, answer the following question: {question}\" + r\"Show your work and reasoning process in <think> </think> tags. And return the final equation in the form \\boxed{answer}, for example <think> reasoning steps </think> Thus the answer is: \\boxed{27}. Think step by step inside <think> tags.\"})\n",
    "    chat_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    return {\n",
    "        \"question\": chat_input\n",
    "    }\n",
    "\n",
    "training_set = training_set.map(generate_CoT_prompt)\n",
    "test_set     = test_set.map(generate_CoT_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def convert_attention_mask(example):\n",
    "# #     if \"attention_mask\" in example:\n",
    "# #         # Assuming 1 indicates a valid token and 0 indicates a masked token:\n",
    "# #         example[\"attention_mask\"] = [float(x) for x in example[\"attention_mask\"]]\n",
    "# #     return example\n",
    "\n",
    "\n",
    "# # def convert_attention_mask(example):\n",
    "# #     if \"attention_mask\" in example:\n",
    "# #         # Convert the list to a torch tensor with float type\n",
    "# #         example[\"attention_mask\"] = torch.tensor(example[\"attention_mask\"], dtype=torch.float)\n",
    "# #     return example\n",
    "\n",
    "# def convert_attention_mask(example):\n",
    "#     if \"attention_mask\" in example:\n",
    "#         # Convert the list to a torch tensor of floats and unsqueeze to have shape [B, 1, 1, key_length]\n",
    "#         example[\"attention_mask\"] = torch.tensor(example[\"attention_mask\"], dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "#     return example\n",
    "\n",
    "# # When mapping your dataset:\n",
    "# training_set = training_set.map(convert_attention_mask)\n",
    "# test_set = test_set.map(convert_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers that Huginn does not currently support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Workaround since Huginn is currently not compatible with quantizing\n",
    "\n",
    "# def get_input_embeddings(self):\n",
    "#     return self.lm_head\n",
    "\n",
    "# def set_input_embeddings(self, value):\n",
    "#     self.lm_head = value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Model Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.wte\n",
      "transformer.prelude\n",
      "transformer.prelude.0\n",
      "transformer.prelude.0.norm_1\n",
      "transformer.prelude.0.attn\n",
      "transformer.prelude.0.attn.Wqkv\n",
      "transformer.prelude.0.attn.proj\n",
      "transformer.prelude.0.norm_2\n",
      "transformer.prelude.0.mlp\n",
      "transformer.prelude.0.mlp.fc\n",
      "transformer.prelude.0.mlp.proj\n",
      "transformer.prelude.0.mlp.nonlin\n",
      "transformer.prelude.0.norm_3\n",
      "transformer.prelude.0.norm_4\n",
      "transformer.prelude.1\n",
      "transformer.prelude.1.norm_1\n",
      "transformer.prelude.1.attn\n",
      "transformer.prelude.1.attn.Wqkv\n",
      "transformer.prelude.1.attn.proj\n",
      "transformer.prelude.1.norm_2\n",
      "transformer.prelude.1.mlp\n",
      "transformer.prelude.1.mlp.fc\n",
      "transformer.prelude.1.mlp.proj\n",
      "transformer.prelude.1.mlp.nonlin\n",
      "transformer.prelude.1.norm_3\n",
      "transformer.prelude.1.norm_4\n",
      "transformer.adapter\n",
      "transformer.core_block\n",
      "transformer.core_block.0\n",
      "transformer.core_block.0.norm_1\n",
      "transformer.core_block.0.attn\n",
      "transformer.core_block.0.attn.Wqkv\n",
      "transformer.core_block.0.attn.proj\n",
      "transformer.core_block.0.norm_2\n",
      "transformer.core_block.0.mlp\n",
      "transformer.core_block.0.mlp.fc\n",
      "transformer.core_block.0.mlp.proj\n",
      "transformer.core_block.0.mlp.nonlin\n",
      "transformer.core_block.0.norm_3\n",
      "transformer.core_block.0.norm_4\n",
      "transformer.core_block.1\n",
      "transformer.core_block.1.norm_1\n",
      "transformer.core_block.1.attn\n",
      "transformer.core_block.1.attn.Wqkv\n",
      "transformer.core_block.1.attn.proj\n",
      "transformer.core_block.1.norm_2\n",
      "transformer.core_block.1.mlp\n",
      "transformer.core_block.1.mlp.fc\n",
      "transformer.core_block.1.mlp.proj\n",
      "transformer.core_block.1.mlp.nonlin\n",
      "transformer.core_block.1.norm_3\n",
      "transformer.core_block.1.norm_4\n",
      "transformer.core_block.2\n",
      "transformer.core_block.2.norm_1\n",
      "transformer.core_block.2.attn\n",
      "transformer.core_block.2.attn.Wqkv\n",
      "transformer.core_block.2.attn.proj\n",
      "transformer.core_block.2.norm_2\n",
      "transformer.core_block.2.mlp\n",
      "transformer.core_block.2.mlp.fc\n",
      "transformer.core_block.2.mlp.proj\n",
      "transformer.core_block.2.mlp.nonlin\n",
      "transformer.core_block.2.norm_3\n",
      "transformer.core_block.2.norm_4\n",
      "transformer.core_block.3\n",
      "transformer.core_block.3.norm_1\n",
      "transformer.core_block.3.attn\n",
      "transformer.core_block.3.attn.Wqkv\n",
      "transformer.core_block.3.attn.proj\n",
      "transformer.core_block.3.norm_2\n",
      "transformer.core_block.3.mlp\n",
      "transformer.core_block.3.mlp.fc\n",
      "transformer.core_block.3.mlp.proj\n",
      "transformer.core_block.3.mlp.nonlin\n",
      "transformer.core_block.3.norm_3\n",
      "transformer.core_block.3.norm_4\n",
      "transformer.coda\n",
      "transformer.coda.0\n",
      "transformer.coda.0.norm_1\n",
      "transformer.coda.0.attn\n",
      "transformer.coda.0.attn.Wqkv\n",
      "transformer.coda.0.attn.proj\n",
      "transformer.coda.0.norm_2\n",
      "transformer.coda.0.mlp\n",
      "transformer.coda.0.mlp.fc\n",
      "transformer.coda.0.mlp.proj\n",
      "transformer.coda.0.mlp.nonlin\n",
      "transformer.coda.0.norm_3\n",
      "transformer.coda.0.norm_4\n",
      "transformer.coda.1\n",
      "transformer.coda.1.norm_1\n",
      "transformer.coda.1.attn\n",
      "transformer.coda.1.attn.Wqkv\n",
      "transformer.coda.1.attn.proj\n",
      "transformer.coda.1.norm_2\n",
      "transformer.coda.1.mlp\n",
      "transformer.coda.1.mlp.fc\n",
      "transformer.coda.1.mlp.proj\n",
      "transformer.coda.1.mlp.nonlin\n",
      "transformer.coda.1.norm_3\n",
      "transformer.coda.1.norm_4\n",
      "transformer.ln_f\n",
      "lm_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "quantizationConfig = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True,  quantization_config=quantizationConfig)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "\n",
    "\n",
    "# model.get_input_embeddings = types.MethodType(get_input_embeddings, model)\n",
    "# model.set_input_embeddings = types.MethodType(set_input_embeddings, model)\n",
    "loftq_config = LoftQConfig(loftq_bits = 4)\n",
    "\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Antoi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\lora\\config.py:455: UserWarning: `loftq_config` specified but will be ignored when `init_lora_weights` is not 'loftq'.\n",
      "  warnings.warn(\"`loftq_config` specified but will be ignored when `init_lora_weights` is not 'loftq'.\")\n"
     ]
    }
   ],
   "source": [
    "target_modules = [\n",
    "    \"attn.Wqkv\",\n",
    "    \"attn.proj\",\n",
    "    \"mlp.fc\",\n",
    "    \"mlp.proj\"\n",
    "]\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, # types here: https://github.com/huggingface/peft/blob/v0.7.1/src/peft/utils/peft_types.py#L22\n",
    "    inference_mode=False,  # false in training, true when inferring\n",
    "    ## YOUR SOLUTION HERE ##\n",
    "    loftq_config=loftq_config,\n",
    "    r=16, # Rank of low-rank matrices\n",
    "    lora_alpha=32,  \n",
    "    lora_dropout=0.1,  # Dropout rate, helps prevent overfitting\n",
    "    target_modules=target_modules,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "qlora_model = get_peft_model(model, peft_config)\n",
    "\n",
    "qlora_model = qlora_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare trainer and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='11106468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [       2/11106468 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"./temp_results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    num_generations = 4\n",
    ")\n",
    "\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "      model=qlora_model,\n",
    "      reward_funcs=[format_rewards, accuracy_rewards],\n",
    "      args=training_args,\n",
    "      train_dataset=training_set,\n",
    "      eval_dataset=test_set,\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()\n",
    "\n",
    "trainer.save_model(\"./final_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
